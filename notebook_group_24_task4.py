# -*- coding: utf-8 -*-
"""notebook_group_24_task4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jBpP_snIu_fBnhUU7bK9B7WBG_zKEUgg

#**Predicting Commercial Flight Cancellations Using Apache Spark**
---
Code corresponding to report *Predicting Commercial Flight Cancellations Using Apache Spark*, Task 4 of the course ID2221 Data-Intensive Computing at KTH Royal Institute of Technology. The purpose of the project is to compare machine learning models for predicting whether a flight gets cancelled, using Apache Spark. Data on flights and airports is retrieved from [Harvard Dataverse](https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/HG7NV7).

**30th of October, 2022**

**Authors: Group 24**
*   Daniel Rubio
*   Yasaman Pazhoolideh
*   Sabine Warringa
---
**Instruction for executing the code**: 
To run this is stand-alone notebook and replicate the results presented in the written report, place it in a folder with subdirectory `Data`. This `Data` folder should contain four datafiles: `"2005.csv.bz2"`, `"2006.csv.bz2"`, `"2007.csv.bz2"` and `"airports.csv"`, downloaded from [Harvard Dataverse](https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/HG7NV7). Upon executing, the notebook will ask the user for the path to the `root` folder. The results are saved in a new output folder in the `root` directory. Hence, upon execution, the directory should look like illustrated below.
```
root
│─── notebook_group_24_task4.ipynb (this notebook)
│─── Data  
│   │─── 2005.csv.bz2
│   │─── 2006.csv.bz2
│   │─── 2007.csv.bz2
│   │─── airports.csv

```

---

The required packages are `pandas` and `pyspark`. The notebook checks whether these packages are available and, if not, installs them via `pip`. This notebook has been executed in Google Colab using `python` version 3.7.15, `pyspark` with Spark 3.3.1, `pandas` version 1.3.5.
"""

# Ask for user input
colab = input("Are you using Google Colab? (y/n) ")
if colab == "y":
  print("Mounting Drive")
  from google.colab import drive
  drive.mount('/content/drive')


path_root = input("Path to root folder: ")
print("Ready.")

"""## 0. Imports, Spark session initialization, and folder initalization"""

# Auxiliary for printing runtime
from datetime import datetime as dt
start0 = dt.now()

# Install pyspark and pandas
!pip install pyspark
!pip install pandas

# Imports
import pandas as pd
from matplotlib import pyplot as plt
import numpy as np

import warnings
warnings.filterwarnings("ignore")

import pyspark
from pyspark.sql import SparkSession
from pyspark.sql.types import FloatType, DoubleType, ArrayType
from pyspark.sql.functions import isnan, when, count, col, UserDefinedFunction, array
from pyspark.ml import Pipeline
from pyspark.ml.functions import vector_to_array
from pyspark.ml.feature import MinMaxScaler, OneHotEncoder, VectorAssembler, StringIndexer, Bucketizer
from pyspark.ml.classification import NaiveBayes, DecisionTreeClassifier, LogisticRegression, RandomForestClassifier
import pyspark.sql.functions as f
import os


# Setting random seed
import random
random.seed(1999)

"""This project uses [Apache Spark PySpark](https://https://spark.apache.org/docs/latest/api/python/), version 3.3.0. The Spark session is initialized on a local master with as many workers as logical cores on the machine (`master("local[*]")`). It uses [pandas](https://https://pandas.pydata.org/) for visualizations."""

# Create a Spark Session
spark = SparkSession.builder.master("local[*]").config("spark.driver.memory", "10g").getOrCreate()

# Check Spark Session Information
spark

time_now = dt.now().strftime("%d%m%Y_%H%M_") # identifier of run

output_folder = path_root + "/Output/" + time_now

if not(os.path.isdir(path_root + "/Output/")):
  os.mkdir(path_root + "/Output")
os.mkdir(output_folder)

# Display runtime of this section
end0 = dt.now()
d0 = end0-start0

print("runtime section 0")
print(d0)

"""## 1. Loading data into a DataFrame

Data is loaded into distributed Spark DataFrames. The schema is automatically inferred. Each year is in a separate file and initially loaded into three DataFrames (`df_2005`, `df_2006`, `df_2007`). These DataFramed are then combined into one `df`. Moreover, information on airports is loaded into `df_airports`.
"""

# Runtime
start1 = dt.now()

# Load data
path = path_root + "/Data/"
path_2005 = path + "2005.csv.bz2"
path_2006 = path + "2006.csv.bz2"
path_2007 = path + "2007.csv.bz2"
path_airports = path + "airports.csv"

df_2005 = spark.read.csv(path_2005, header=True)
df_2006 = spark.read.csv(path_2006, header=True)
df_2007 = spark.read.csv(path_2007, header=True)
df_airports = spark.read.csv(path_airports, header=True)



# For debugging purposes
test = False

if test:
  percentage = 0.05
  df = df_2005.union(df_2006).union(df_2007)
  df_temp = df.withColumn("Cancelled",df.Cancelled.cast('int'))
  df = df_temp.sampleBy("Cancelled", fractions={0: percentage, 1: percentage})
  print('Loaded test data')
else:
  df = df_2005.union(df_2006).union(df_2007)
  print('Loaded full dataset')

df.printSchema()

# Displaying runtime of this section
end1 = dt.now()
d1 = end1-start1

print(f"Runtime Section 1: {d1}")

"""## 2. Data Exploration and Processing"""

# Runtime
start2 = dt.now()

"""The schema of `df` was automatically inferred by PySpark, but some adjustments have to be made and not all columns are used as is. More specifically:


*   `Year` is excluded
*   `Month` is used to generate the categorical column `season`.
* `DayofMonth` is not used as the `season` variable is an approximation and does not take the day into account
*    `DayOfWeek` is used to generate `weekday` as a boolean value
*   `CRSArrTime` is excluded as this information is captured in the `CRSDepDayPart` and `CRSElapsedTime` variables
*   `UniqueCarrier` is excluded as converting this in categorical values would result in too many categories
*   `FlightNum` is excluded as converting this in categorical values would result in too many categories
*   `TailNum`  is excluded as converting this in categorical values would result in too many categories
*   `CRSElapsedTime` and `Distance` are included in the model (scaled).
*   `Origin` and `Dest` are used together with the airport information dataset (`df_airports`) to generate an `angle` column. This is then transofrmed into the `direction` column respresenting the direction of the flight (NW, NE, SW, etc.)
*  `Cancelled` is the boolean target variable
*   `DepTime`, `ArrTime`, `ActualElapsedTime`, `AirTime`, `ArrDelay`, `DepDelay`, `TaxiIn`, `TaxiOut`, `CancellationCode`, `Diverted`, `CarrierDelay`, `WeatherDelay`, `NasDelay`, `SecurityDelay`, and `LateAircraftDelay` are excluded as this information is not applicable in case a flight is cancelled and is unknown before departure or cancellation.

### Processing the flights data
"""

# Delete variables
cols_to_delete = ["Year", "DepTime", "ArrTime", "CRSArrTime", "ActualElapsedTime", "AirTime", "ArrDelay", "DepDelay", "TaxiIn", "TaxiOut", "CancellationCode", "Diverted", "CarrierDelay", "WeatherDelay", "NASDelay", "SecurityDelay", "LateAircraftDelay", "UniqueCarrier", "FlightNum", "TailNum", "DayofMonth", "CRSDepTime"]
df = df.drop(*cols_to_delete)

df.printSchema()

"""#### Deleting missing values

The data contains some missing values that can influence the performance of our models, or cannot be handled by the models. As the dataset is imbalanced, it is important to know whether the missing values are uniformly distributed over the classes (cancelled/not cancelled).
"""

# Display percentage of missing values per cancelled/not cancelled
df_cancelled_grouped = df.groupBy("Cancelled").agg(*[(count(when(col(c).like('NA') | col(c).isNull(), c))/count(c)*100).alias(c+"_perc") for c in df.columns]).cache()
print("Percentage of missing values by Cancelled")
df_cancelled_grouped.show()

# Loading into pandas dataframe
df_pd_cancelled = df_cancelled_grouped.toPandas()
df_pd_cancelled = df_pd_cancelled.set_index('Cancelled').sort_index() # 0 = not cancelled, 1 = cancelled
df_pd_cancelled.drop(columns=["Cancelled_perc"], inplace=True)

df_pd_cancelled.rename(columns={"Month_perc": "Month", 
                                "DayOfWeek_perc": "Day of week",
                                "CRSElapsedTime_perc": "CRS Elapsed Time",
                                "Origin_perc": "Origin",
                                "Dest_perc": "Destination",
                                "Distance_perc": "Distance"}, inplace=True)

print("Proportion of missing values")
display(df_pd_cancelled)

# Visualization
df_pd_cancelled.T.plot(
        kind='bar',
        stacked=False,
        title="Proportion of missing values",
        ylabel="% of total")
plt.legend(["Not cancelled", "Cancelled"])

"""The percentage of missing values is low, and approximately the same for cancelled and not-cancelled flights. These values are therefore excluded from the model."""

def remove_null_entries (df):
  for c in df.columns:
    df = df.filter(~(col(c).like('NA') | col(c).isNull()))
  return df
  
df = remove_null_entries(df)
df = df.cache()

"""#### Type casting
Before further processing, some columns need to be casted to a different type. The column `Cancelled_double` is needed in Section 3 to calculate metrics.

"""

df = df.withColumn("DayOfWeek", df["DayOfWeek"].cast('int')) \
          .withColumn("CRSElapsedTime", df["CRSElapsedTime"].cast('float')) \
          .withColumn("Distance", df["Distance"].cast('float')) \
          .withColumn("Cancelled_double", df["Cancelled"].cast('float')) \
          .withColumn("Cancelled", df["Cancelled"].cast('int'))

"""#### Adding the `season` variable

The `season` variable is an approximation of the season in which this flight occurred. Here, we assume:
*   `winter` is December (12), January (1), February (2)
*   `spring` is March (3), April (4), May (5)
* `summer` is June (6), July (7), August (8)
* `autumn` is September (9), October (10), November (11)
"""

df = df.withColumn("season", 
                   when(col("Month")==1, "winter")
                   .when(col("Month")==2, "winter")
                   .when(col("Month")==3, "spring")
                   .when(col("Month")==4, "spring")
                   .when(col("Month")==5, "spring")
                   .when(col("Month")==6, "summer")
                   .when(col("Month")==7, "summer")
                   .when(col("Month")==8, "summer")
                   .when(col("Month")==9, "autumn")
                   .when(col("Month")==10, "autumn")
                   .when(col("Month")==11, "autumn")
                   .when(col("Month")==12, "winter")
                   )

"""#### Adding the `weekday` variable
`weekday` represents whether the flight departed on a weekday or a weekend (`DayofWeek==6` or `DayofWeek==7`).
"""

df = df.withColumn("weekday", 
                   when(col("DayofWeek")==6, 0) # weekend
                   .when(col("DayofWeek")==7, 0) # weekend
                   .otherwise(1)
)

"""### Creating `angle` variable (later: `direction`). 
The `direction` variable respresents which direction the flight was in. First,the angle between the origin and destination airport is calculated and stored in column `angle`. Later in this section, these angles are mapped to directions on an 8-wind compass.
To generate `angle`, the flight dataset (`df`) is joined twice with the airport dataset (`df_airports`): once with the origin airport as primary key, and once with the destination airport as primary key. This results in the `df_full` dataset. Then, the coordinates of the origin and destination airport are used to calculate the angle.
More specifically, the information used from the airport dataset (`df_airports`) is:
* `iata` is used as a key
* `airport`, `city`, `state`, and `country` are deleted and not used
* `lat` and `long` are used as coordinates for this airport
"""

df_airports.printSchema()

# Drop columns that are not needed
df_airports = df_airports.drop(*["airport", "city", "state", "country"])

# Get origin information
df_o = df.join(df_airports, on=df["Origin"]==df_airports["iata"], how='left')
df_o = df_o.withColumnRenamed("lat", "lat_origin")
df_o = df_o.withColumnRenamed("long", "long_origin")

# Drop iata column
df_o = df_o.drop("iata")

# Get destination information
df_full = df_o.join(df_airports, on=df_o["Dest"]==df_airports["iata"], how='left')
df_full = df_full.withColumnRenamed("lat", "lat_dest")
df_full = df_full.withColumnRenamed("long", "long_dest")

# Cast datatypes
df_full = df_full.withColumn("lat_origin", df_full["lat_origin"].cast("double")) \
                  .withColumn("long_origin", df_full["long_origin"].cast("double")) \
                  .withColumn("lat_dest", df_full["lat_dest"].cast("double")) \
                  .withColumn("long_dest", df_full["long_dest"].cast("double"))

"""Any missing values from the joins are deleted."""

# Check for missing values %
df_full.groupBy("Cancelled").agg(*[(count(when(col(c).like('NA') | col(c).isNull(), c))/count(c)*100).alias(c+"_perc") for c in df_full.columns]).show()

df_full = remove_null_entries(df_full)
df_full.cache()

df_full.printSchema()

"""Now, all information needed to calculate the angle is present in `df_full`."""

def getAngle(lat1, long1, lat2, long2):
    
    long_distance = (long2 - long1)
    
    x = f.cos(f.radians(lat2)) * f.sin(f.radians(long_distance))
    y = f.cos(f.radians(lat1)) * f.sin(f.radians(lat2)) - f.sin(f.radians(lat1)) * f.cos(f.radians(lat2)) * f.cos(f.radians(long_distance))

    angle = f.atan2(x,y)
    degr = f.degrees(angle)
    
    return degr

# Apply the function to create the Angle column
df_full = df_full.withColumn("Angle", getAngle(df_full["lat_origin"], 
                                                    df_full["long_origin"], 
                                                    df_full["lat_dest"],
                                                    df_full["long_dest"]))

# Drop columns that are no longer needed
df_full = df_full.drop(*["Origin", "Dest", "lat_origin", "long_origin", "lat_dest", "long_dest", "iata"])

df_full.printSchema()

df_full = df_full.cache()

"""### Weighting data

The dataset is very unbalanced, with many not cancelled flights `Cancelled = 0` and a few cancelled flights `Cancelled = 1`. Therefore, weights will be used in the machine learning models as a solution. Weights will be calculated as in https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html when using "balanced mode" (n_samples / (n_classes * np.bincount(y))) (weights are inversely proportional to class frequencies)
"""

# Counting and displaying the proportion of cancelled in the dataset
df_counts = df_full.groupBy("Cancelled").agg({"Cancelled": "count"}).cache()

df_pd_counts = df_counts.toPandas()
df_pd_counts["Cancelled"] = df_pd_counts["Cancelled"].map({0:"Not cancelled", 1:"Cancelled"})
df_pd_counts = df_pd_counts.set_index('Cancelled')

print("Number of cancelled flights")
display(df_pd_counts)

# Visualization
ax = df_pd_counts.plot(kind="bar", legend=None, xlabel="", title="Number of fligths per cancelled/not cancelled")
ax.ticklabel_format(style='plain', axis='y')

"""Creating a pyspark DataFrame with weights, to be joined with the full dataset"""

n = df_pd_counts['count(Cancelled)'].sum()
df_weights = df_full.groupby("Cancelled").agg((n/(2*count("Cancelled"))).alias("Weight"))
df_weights = df_weights.withColumnRenamed("Cancelled", "Cancelled_weight").cache()
df_weights.show()

"""The weights (`df_weights`) are joined with the full dataset (`df_full`) to add a weight column."""

df_full.printSchema()

df_weights.printSchema()

# Join
df_full = df_full.join(df_weights, df_full.Cancelled == df_weights.Cancelled_weight, "leftouter")
df_full.printSchema()

"""### Further processing in pipeline

The data is now prepared to be fed into an [ML Pipeline](https://https://spark.apache.org/docs/latest/ml-pipeline.html) for further processing. In this pipeline:
* categorical variables are one-hot-encoded
* the categorical `direction` is created from the `angle` column 
* continuous variables are scaled
"""

# Pipeline stages
pipeline_stages = []

# Bucketizing angle into direction
BucketizerDirection = Bucketizer(splits=[-180, -135, -90, -45, 0, 45, 90, 135, 180], inputCol = "Angle", outputCol="Direction")
ohe_direction = OneHotEncoder(inputCol="Direction", outputCol="Direction_ohe")
pipeline_stages += [BucketizerDirection]
pipeline_stages += [ohe_direction]

# Dummies for season
string_season = StringIndexer(inputCol="season", outputCol="season_index")
pipeline_stages += [string_season]

ohe_season = OneHotEncoder(inputCol="season_index", outputCol="season_ohe")
pipeline_stages += [ohe_season]

# Min max scaling: CRSElapsedTime
assembler_minmax_time = VectorAssembler(inputCols=["CRSElapsedTime"], outputCol="CRSElapsedTime_feature")
pipeline_stages += [assembler_minmax_time]

minmax_time = MinMaxScaler(inputCol="CRSElapsedTime_feature", outputCol="CRSElapsedTime_scaled")
pipeline_stages += [minmax_time]

# Min max scaling: Distance
assembler_minmax_dist = VectorAssembler(inputCols=["Distance"], outputCol="Distance_feature")
pipeline_stages += [assembler_minmax_dist]

minmax_dist = MinMaxScaler(inputCol="Distance_feature", outputCol="Distance_scaled")
pipeline_stages += [minmax_dist]

"""### Train-test split and running the pipeline
The data is split into a 90/10 train-test set, where the proportions of cancelled/not-cancelled flights are kept from the full dataset.
"""

df_full.printSchema()

"""The data processing pipeline is fit on the train set and used to transform both the train and test set."""

# Maintain proportion of Cancelled/ NotCancelled in train and test
y_label = "Cancelled"
prop_train_test = 0.90
df_train = df_full.sampleBy(y_label, fractions={0: prop_train_test, 1: prop_train_test})
df_test = df_full.subtract(df_train)

# Run pipeline
pipeline = Pipeline(stages=pipeline_stages)
pipeline_model = pipeline.fit(df_train)
df_train_transformed = pipeline_model.transform(df_train).cache()
df_test_transformed = pipeline_model.transform(df_test).cache()

"""### Visualizations"""

df_train_transformed.printSchema()

df_train.head()

df_pd_season = df_train_transformed.select("Cancelled", "season", "season_index").groupBy("season").avg().toPandas()
df_pd_season.sort_values(by="avg(season_index)", inplace=True)

df_pd_weekday = df_train_transformed.select("Cancelled", "weekday").groupBy("weekday").avg().toPandas()
df_pd_weekday.sort_values(by="weekday", inplace=True, ascending=False)
df_pd_weekday["weekday"] = df_pd_weekday["weekday"].map({1: "weekday", 0:"weekend"})

df_pd_direction = df_train_transformed.select("Cancelled", "direction").groupBy("direction").avg().toPandas()
df_pd_direction["direction_str"] = df_pd_direction["direction"].map({0.0: "SSW",
                                                                 1.0: "WSW",
                                                                 2.0: "WNW",
                                                                 3.0: "NNW",
                                                                 4.0: "NNE",
                                                                 5.0: "ENE",
                                                                 6.0: "ESE",
                                                                 7.0: "SSE"})

df_pd_direction["direction_angle"] = df_pd_direction["direction"].map({0.0: 157.5,
                                                                 1.0: 202.5,
                                                                 2.0: 247.5,
                                                                 3.0: 292.5,
                                                                 4.0: 337.5,
                                                                 5.0: 22.5,
                                                                 6.0: 67.5,
                                                                 7.0: 112.5})

# Breaking up into buckets
tick_labels = ['WNW','NNW','WSW','ESE','SSE','SSW','NNE','ENE']

plt.figure(figsize=(5,5))
ax = plt.subplot(projection='polar')
width = np.pi / 4.1
ax.bar(df_pd_direction['direction_angle']*np.pi/180, df_pd_direction['avg(Cancelled)'], width=width, tick_label = tick_labels)
plt.title("Percentage of cancelled flights per direction")

plt.show()

fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(7,5), sharey=True, gridspec_kw={'width_ratios': [1, 2]})
width=0.8

fig.suptitle("Percentage of cancelled flights per category")

ax1.bar(df_pd_weekday["weekday"], df_pd_weekday["avg(Cancelled)"], width=width)
ax2.bar(df_pd_season["season"], df_pd_season["avg(Cancelled)"], width=width)

plt.show()

"""### Preparing the data for machine learning models
The machine learning models in Section 3 take a vectorized set of columns as input, as well as a target column and optionally a weight column.
"""

# Set input columns, weight column, and target column
input_cols = ["Direction_ohe", "weekday", "season_ohe", "CRSElapsedTime_scaled", "Distance_scaled"]
weight_col = "Weight"
target_col = "Cancelled"
target_col_double = "Cancelled_double" # Used to calculate metrics

# Package input columns as feature vector
assembler_modelinput = VectorAssembler(inputCols=input_cols, outputCol="features")
df_train_transformed = assembler_modelinput.transform(df_train_transformed).cache()
df_test_transformed = assembler_modelinput.transform(df_test_transformed).cache()

end2 = dt.now()
d2 = end2-start2
print(f"Runtime Section 2: {d2}")

"""## 3. Machine Learning Models & Gridsearch

This section applies gridsearch over several weighted and unweighted models. It displays the results of each model in a table, which is also saved as a `.csv` file in the specified output folder. Also, it saves for each model a visualization of the confusion matric as a `.jpg` image in the same output folder.
"""

start3 = dt.now()

"""### Methods to obtain metrics

Metrics to be calculated by the models
"""

import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
from pyspark.mllib.evaluation import MulticlassMetrics
from pyspark.mllib.evaluation import BinaryClassificationMetrics

def save_cf(cm, path=None):
  matrix = cm/(cm.sum(axis=0)+0.0000001) # to prevent nulls
  vmin = 0
  vmax = 1
  off_diag_mask = np.eye(*matrix.shape, dtype=bool)

  fig = plt.figure()
  sns.heatmap(matrix, annot=True, mask=~off_diag_mask, cmap='Greens', vmin=vmin, vmax=vmax)
  sns.heatmap(matrix, annot=True, mask=off_diag_mask, cmap='OrRd', vmin=vmin, vmax=vmax, cbar_kws=dict(ticks=[]))
  plt.savefig(path)
  plt.close(fig)

def calculate_balanced_acc(cm):
  tp = cm[0,0]
  fn = cm[0,1]
  fp = cm[1,0]
  tn = cm[1,1]
  sensitivity = tp/(tp+fn)
  specificity = tn/(tn+fp)
  balanced_acc = (sensitivity+specificity)/2
  return balanced_acc

def getMetrics(predictions, path=None):
  metrics = MulticlassMetrics(predictions.rdd)
  bmetrics = BinaryClassificationMetrics(predictions.rdd)

  # Calculate metrics
  conf_matrix = metrics.confusionMatrix().toArray()
  weightedF1 = metrics.weightedFMeasure(2.0)
  precision0 = metrics.precision(0.0)
  precision1 = metrics.precision(1.0)
  recall0 = metrics.recall(0.0)
  recall1 = metrics.recall(1.0)

  # Binary metrics
  areaUnderPR = bmetrics.areaUnderPR
  areaUnderROC = bmetrics.areaUnderROC
  balanced_acc = calculate_balanced_acc(conf_matrix)

  # Save results to dictionary
  res = {}
  res["conf_matrix"] = conf_matrix
  res["weightedF1"] = weightedF1
  res["precision0"] = precision0
  res["precision1"] = precision1
  res["recall0"] = recall0
  res["recall1"] = recall1
  res["areaUnderPR"] = areaUnderPR
  res["areaUnderROC"] = areaUnderROC
  res["balanced_acc"] = balanced_acc

  if path != None:
    # Save confusion matrix
    save_cf(conf_matrix, path)
  
  return res

"""### Gridsearch
This section performs a hyperparameter tuning using gridsearch for three classification models: decision tree classifier, random forest classifier, and logistic regression classifier. All models are implemented using Spark's MLlib functions. For each model, the gridsearch includes a parameter for whether the data is weighted or not. Some additional parameters are tuned for each model too. 

The metrics for each iteration of the gridsearch are computed using the `getMetrics()` function. A visualization of the corresponding confusion matrix is saved as an image in the output folder. The results are collected in a Pandas Dataframe for each model (decision tree `dtc`, random forest `rf`, and logistic regression `lr`), displayed on screen, and saved as a `.csv` file in the output folder.

#### Decision Tree Classifier
Uses PySparks `DecisionTreeClassifier()` model and performs gridsearch over the following parameters
* Weighted or unweighted: `weightCol` is either empty or set to the weight column in the data
* `maxDepth` in `[5, 10, 15]`
* `maxBin` in `[32, 64]`
"""

params_weighted = [weight_col, ""]
params_depth = [5, 10, 15]
params_bin = [32, 64]

counter = 1
model= "dtc"

dic_results = {"id":[],
               "param_weighted":[],
               "param_depth":[],
               "param_bin":[],
               'runtime':[],
               'conf_matrix': [], 
               'weightedF1': [], 
               'precision0': [], 
               'precision1': [],
               'recall0': [], 
               'recall1': [], 
               'areaUnderPR': [], 
               'areaUnderROC': [], 
               'balanced_acc': []
               }

for w in params_weighted:
  for d in params_depth:
    for b in params_bin:

      # Set information in dictionary
      id = model+str(counter)
      dic_results["id"].append(id)
      dic_results["param_weighted"].append(w)
      dic_results["param_depth"].append(d)
      dic_results["param_bin"].append(b)
      

      # Run model and record runtime
      start = dt.now()

      dtc = DecisionTreeClassifier(featuresCol="features", labelCol=target_col, weightCol=w, 
                                   maxBins=b, maxDepth=d)
      df_pred = dtc.fit(df_train_transformed).transform(df_test_transformed)

      input_metrics = df_pred.select(["prediction", target_col_double, weight_col])
      dic_metrics = getMetrics(input_metrics, path=output_folder + "/" + time_now + id + ".jpg")
      
      for k in dic_metrics:
        dic_results[k].append(dic_metrics[k])
      
      end = dt.now()
      dic_results["runtime"].append(end-start)


      # Increase counter
      counter += 1

pandas_output = pd.DataFrame(dic_results)
pandas_output.to_csv(output_folder + "/" + time_now + model + ".csv")
display(pandas_output)

"""#### Binomial Logistic Regression
Uses PySparks `LogisticRegression()` model and performs gridsearch over the following parameters
* Weighted or unweighted: `weightCol` is either empty or set to the weight column in the data
* `maxIter` in `[100, 250]`
* `fitIntercept` in `[True, False]`
* `regParam` in `[0.0, 0.3, 0.5]`
"""

params_maxiter = [100, 250]
params_weighted = [weight_col, ""]
params_intercept = [True, False]
params_reg = [0.0, 0.3, 0.5]

counter = 1
model= "lr"

dic_results = {"id":[],
               "param_weighted":[],
               "param_maxiter":[],
               "param_intercept":[],
               "param_reg":[],
               'runtime':[],
               'conf_matrix': [], 
               'weightedF1': [], 
               'precision0': [],
               'precision1': [], 
               'recall0': [], 
               'recall1': [], 
               'areaUnderPR': [], 
               'areaUnderROC': [], 
               'balanced_acc': []
               }

for mi in params_maxiter:
  for w in params_weighted:
    for ic in params_intercept:
      for r in params_reg:

        # Set information in dictionary
        id = model+str(counter)
        dic_results["id"].append(id)
        dic_results["param_weighted"].append(w)
        dic_results["param_intercept"].append(ic)
        dic_results["param_reg"].append(r)
        dic_results["param_maxiter"].append(mi)
        

        # Run model and record runtime
        start = dt.now()

        lr = LogisticRegression(featuresCol="features", labelCol=target_col, weightCol=w, 
                                    fitIntercept=ic, regParam=r, maxIter=mi)
        df_pred = lr.fit(df_train_transformed).transform(df_test_transformed)

        input_metrics = df_pred.select(["prediction",target_col_double,weight_col])
        dic_metrics = getMetrics(input_metrics, path=output_folder + "/" + time_now + id + ".jpg")
        
        for k in dic_metrics:
          dic_results[k].append(dic_metrics[k])
        
        end = dt.now()
        dic_results["runtime"].append(end-start)


        # Increase counter
        counter += 1

pandas_output = pd.DataFrame(dic_results)
pandas_output.to_csv(output_folder + "/" + time_now + model + ".csv")
display(pandas_output)

"""#### Random Forest
Uses PySparks `RandomForestClassifier()` model and performs gridsearch over the following parameters
* Weighted or unweighted: `weightCol` is either empty or set to the weight column in the data
* `maxDepth` in `[5, 10, 15]`
* `maxBin` in `[32, 64]`
"""

params_weighted = [weight_col, ""]
params_depth = [5, 10, 15]
params_bin = [32, 64]

counter = 1
model= "rf"

dic_results = {"id":[],
               "param_weighted":[],
               "param_depth":[],
               "param_bin":[],
               'runtime':[],
               'conf_matrix': [], 
               'weightedF1': [], 
               'precision0': [], 
               'precision1': [],
               'recall0': [], 
               'recall1': [], 
               'areaUnderPR': [], 
               'areaUnderROC': [], 
               'balanced_acc': []
               }

for w in params_weighted:
  for d in params_depth:
    for b in params_bin:

      # Set information in dictionary
      id = model+str(counter)
      dic_results["id"].append(id)
      dic_results["param_weighted"].append(w)
      dic_results["param_depth"].append(d)
      dic_results["param_bin"].append(b)
      

      # Run model and record runtime
      start = dt.now()

      rf = RandomForestClassifier(featuresCol="features", labelCol=target_col, weightCol=w, 
                                   maxBins=b, maxDepth=d)
      df_pred = rf.fit(df_train_transformed).transform(df_test_transformed)

      input_metrics = df_pred.select(["prediction",target_col_double,weight_col])
      dic_metrics = getMetrics(input_metrics, path=output_folder + "/" + time_now + id + ".jpg")
      
      for k in dic_metrics:
        dic_results[k].append(dic_metrics[k])
      
      end = dt.now()
      dic_results["runtime"].append(end-start)


      # Increase counter
      counter += 1

pandas_output = pd.DataFrame(dic_results)
pandas_output.to_csv(output_folder + "/" + time_now + model + ".csv")
display(pandas_output)

end3 = dt.now()
d3 = end3-start3

print("runtime section 3")
print(d3)